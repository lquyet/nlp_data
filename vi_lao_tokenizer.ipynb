{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport os\nimport pathlib\nimport re\nimport string\nimport sys\nimport tempfile\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow_text as text\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.get_logger().setLevel('ERROR')\npwd = pathlib.Path.cwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf nlp_data\n!git clone https://github.com/lquyet/nlp_data.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/nlp_data/v5/train2023_cleaned.vi\", \"r\", encoding=\"utf-8\") as f:\n    vi = f.readlines()\n\nwith open(\"/kaggle/working/nlp_data/v5/train2023_cleaned.lo\", \"r\", encoding=\"utf-8\") as f:\n    lo = f.readlines()\n    \ntrain_examples = tf.data.Dataset.from_tensor_slices((lo, vi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lo, vi in train_examples.take(1):\n  print(\"Laos: \", lo.numpy().decode('utf-8'))\n  print(\"Viet:   \", vi.numpy().decode('utf-8'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_vi = train_examples.take(100000).map(lambda lo, vi: vi)\ntrain_lo = train_examples.take(100000).map(lambda lo, vi: lo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer_params=dict(lower_case=True)\nreserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n\nbert_vocab_args = dict(\n    # The target vocabulary size\n    vocab_size = 32000,\n    # Reserved tokens that must be included in the vocabulary\n    reserved_tokens=reserved_tokens,\n    # Arguments for `text.BertTokenizer`\n    bert_tokenizer_params=bert_tokenizer_params,\n    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n    learn_params={},\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlo_vocab = bert_vocab.bert_vocab_from_dataset(\n    train_lo.batch(1000).prefetch(2),\n    **bert_vocab_args\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lo_vocab[:10])\nprint(lo_vocab[100:110])\nprint(lo_vocab[1000:1010])\nprint(lo_vocab[-10:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_vocab_file(filepath, vocab):\n  with open(filepath, 'w') as f:\n    for token in vocab:\n      print(token, file=f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_vocab_file('lo_vocab.txt', lo_vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvi_vocab = bert_vocab.bert_vocab_from_dataset(\n    train_vi.batch(1000).prefetch(2),\n    **bert_vocab_args\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vi_vocab[:10])\nprint(vi_vocab[100:110])\nprint(vi_vocab[1000:1010])\nprint(vi_vocab[-10:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_vocab_file('vi_vocab.txt', vi_vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls *.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lo_tokenizer = text.BertTokenizer('lo_vocab.txt', **bert_tokenizer_params)\nvi_tokenizer = text.BertTokenizer('vi_vocab.txt', **bert_tokenizer_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lo_examples, vi_examples in train_examples.batch(3).take(1):\n  for ex in vi_examples:\n    print(ex.numpy().decode(\"utf-8\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the examples -> (batch, word, word-piece)\ntoken_batch = vi_tokenizer.tokenize(vi_examples)\n# Merge the word and word-piece axes -> (batch, tokens)\ntoken_batch = token_batch.merge_dims(-2,-1)\n\nfor ex in token_batch.to_list():\n  print(ex)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lookup each token id in the vocabulary.\ntxt_tokens = tf.gather(vi_vocab, token_batch)\n# Join with spaces.\ntf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = vi_tokenizer.detokenize(token_batch)\ntf.strings.reduce_join(words, separator=' ', axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\nEND = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n\ndef add_start_end(ragged):\n  count = ragged.bounding_shape()[0]\n  starts = tf.fill([count,1], START)\n  ends = tf.fill([count,1], END)\n  return tf.concat([starts, ragged, ends], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = vi_tokenizer.detokenize(add_start_end(token_batch))\ntf.strings.reduce_join(words, separator=' ', axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleanup_text(reserved_tokens, token_txt):\n  # Drop the reserved tokens, except for \"[UNK]\".\n  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n  bad_token_re = \"|\".join(bad_tokens)\n    \n  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n\n  # Join them into strings.\n  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n\n  return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vi_examples.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_batch = vi_tokenizer.tokenize(vi_examples).merge_dims(-2,-1)\nwords = vi_tokenizer.detokenize(token_batch)\nwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleanup_text(reserved_tokens, words).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomTokenizer(tf.Module):\n  def __init__(self, reserved_tokens, vocab_path):\n    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n    self._reserved_tokens = reserved_tokens\n    self._vocab_path = tf.saved_model.Asset(vocab_path)\n\n    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n    self.vocab = tf.Variable(vocab)\n\n    ## Create the signatures for export:   \n\n    # Include a tokenize signature for a batch of strings. \n    self.tokenize.get_concrete_function(\n        tf.TensorSpec(shape=[None], dtype=tf.string))\n    \n    # Include `detokenize` and `lookup` signatures for:\n    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n    #   * `RaggedTensors` with shape [batch, tokens]\n    self.detokenize.get_concrete_function(\n        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n    self.detokenize.get_concrete_function(\n          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n\n    self.lookup.get_concrete_function(\n        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n    self.lookup.get_concrete_function(\n          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n\n    # These `get_*` methods take no arguments\n    self.get_vocab_size.get_concrete_function()\n    self.get_vocab_path.get_concrete_function()\n    self.get_reserved_tokens.get_concrete_function()\n    \n  @tf.function\n  def tokenize(self, strings):\n    enc = self.tokenizer.tokenize(strings)\n    # Merge the `word` and `word-piece` axes.\n    enc = enc.merge_dims(-2,-1)\n    enc = add_start_end(enc)\n    return enc\n\n  @tf.function\n  def detokenize(self, tokenized):\n    words = self.tokenizer.detokenize(tokenized)\n    return cleanup_text(self._reserved_tokens, words)\n\n  @tf.function\n  def lookup(self, token_ids):\n    return tf.gather(self.vocab, token_ids)\n\n  @tf.function\n  def get_vocab_size(self):\n    return tf.shape(self.vocab)[0]\n\n  @tf.function\n  def get_vocab_path(self):\n    return self._vocab_path\n\n  @tf.function\n  def get_reserved_tokens(self):\n    return tf.constant(self._reserved_tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizers = tf.Module()\ntokenizers.lo = CustomTokenizer(reserved_tokens, 'lo_vocab.txt')\ntokenizers.vi = CustomTokenizer(reserved_tokens, 'vi_vocab.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'btl_nlp_lao_viet'\ntf.saved_model.save(tokenizers, model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reloaded_tokenizers = tf.saved_model.load(model_name)\nreloaded_tokenizers.vi.get_vocab_size().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = reloaded_tokenizers.vi.tokenize(['Xin chao Viet Nam'])\ntokens.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tokens = reloaded_tokenizers.vi.lookup(tokens)\ntext_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round_trip = reloaded_tokenizers.vi.detokenize(tokens)\n\nprint(round_trip.numpy()[0].decode('utf-8'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r {model_name}.zip {model_name}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!du -h *.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}